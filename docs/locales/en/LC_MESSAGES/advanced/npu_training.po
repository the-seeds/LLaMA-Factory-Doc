# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2024, LlamaFactory team.
# This file is distributed under the same license as the LLaMA Factory
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2025.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: LLaMA Factory \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-03-05 01:10+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: en\n"
"Language-Team: en <LL@li.org>\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.16.0\n"

#: ../../source/advanced/npu_training.rst:2
msgid "NPU训练"
msgstr "NPU Training"

#: ../../source/advanced/npu_training.rst:4
msgid "本文档介绍如何在华为昇腾 NPU 上进行 LLaMA-Factory 模型训练，包括设备支持、训练范式、分布式策略以及性能优化等内容。"
msgstr "This document describes how to perform LLaMA-Factory model training on Huawei Ascend NPU, including device support, training paradigms, distributed strategies, and performance optimization."

#: ../../source/advanced/npu_training.rst:6
msgid "支持设备"
msgstr "Supported Devices"

#: ../../source/advanced/npu_training.rst:9
msgid "LLaMA-Factory 当前已适配以下昇腾 NPU 设备："
msgstr "LLaMA-Factory currently supports the following Ascend NPU devices:"

#: ../../source/advanced/npu_training.rst:11
msgid "**Atlas A2 训练系列**"
msgstr "**Atlas A2 Training Series**"

#: ../../source/advanced/npu_training.rst:12
msgid "**Atlas 800I A2 推理系列**"
msgstr "**Atlas 800I A2 Inference Series**"

#: ../../source/advanced/npu_training.rst:15
msgid "支持功能"
msgstr "Supported Features"

#: ../../source/advanced/npu_training.rst:24
msgid "功能"
msgstr "Feature"

#: ../../source/advanced/npu_training.rst:25
msgid "支持情况"
msgstr "Support Status"

#: ../../source/advanced/npu_training.rst:26
msgid "**训练范式**"
msgstr "**Training Paradigm**"

#: ../../source/advanced/npu_training.rst:27
msgid "PT"
msgstr "PT"

#: ../../source/advanced/npu_training.rst:28
msgid "已支持"
msgstr "Supported"

#: ../../source/advanced/npu_training.rst:30
msgid "SFT"
msgstr "SFT"

#: ../../source/advanced/npu_training.rst:33
msgid "RM"
msgstr "RM"

#: ../../source/advanced/npu_training.rst:36
msgid "DPO"
msgstr "DPO"

#: ../../source/advanced/npu_training.rst:38
msgid "**参数范式**"
msgstr "**Parameter Paradigm**"

#: ../../source/advanced/npu_training.rst:39
msgid "Full"
msgstr "Full"

#: ../../source/advanced/npu_training.rst:42
msgid "Freeze"
msgstr "Freeze"

#: ../../source/advanced/npu_training.rst:45
msgid "LoRA"
msgstr "LoRA"

#: ../../source/advanced/npu_training.rst:47
msgid "**模型合并**"
msgstr "**Model Merging**"

#: ../../source/advanced/npu_training.rst:48
msgid "LoRA权重合并"
msgstr "LoRA Weight Merging"

#: ../../source/advanced/npu_training.rst:50
msgid "**分布式**"
msgstr "**Distributed**"

#: ../../source/advanced/npu_training.rst:51
msgid "DDP"
msgstr "DDP"

#: ../../source/advanced/npu_training.rst:54
msgid "FSDP"
msgstr "FSDP"

#: ../../source/advanced/npu_training.rst:57
msgid "DeepSpeed"
msgstr "DeepSpeed"

#: ../../source/advanced/npu_training.rst:59
msgid "**加速**"
msgstr "**Acceleration**"

#: ../../source/advanced/npu_training.rst:60
msgid "融合算子"
msgstr "Fused Operators"

#: ../../source/advanced/npu_training.rst:61
msgid "当前仅支持NPU FA融合算子"
msgstr "Currently only NPU FA fused operators are supported"

#: ../../source/advanced/npu_training.rst:65
msgid "除特别说明外，NPU 的使用方式与 GPU 保持一致，无需额外配置。以下将重点介绍 NPU 特定的配置和注意事项。"
msgstr "Unless otherwise specified, NPU usage is consistent with GPU usage and requires no additional configuration. The following will focus on NPU-specific configurations and considerations."

#: ../../source/advanced/npu_training.rst:67
msgid "分布式训练"
msgstr "Distributed Training"

#: ../../source/advanced/npu_training.rst:70
msgid "单机微调"
msgstr "Single Machine Fine-tuning"

#: ../../source/advanced/npu_training.rst:73
msgid "本节介绍如何在单机环境下使用 Docker 容器进行 NPU 模型微调。LLaMA-Factory 提供了两种 Docker 部署方式。"
msgstr "This section describes how to fine-tune NPU models using Docker containers in a single machine environment. LLaMA-Factory provides two Docker deployment methods."

#: ../../source/advanced/npu_training.rst:75
msgid "**方法一：使用 Docker Compose（推荐）**"
msgstr "**Method 1: Using Docker Compose (Recommended)**"

#: ../../source/advanced/npu_training.rst:84
msgid "**方法二：使用 Docker Run**"
msgstr "**Method 2: Using Docker Run**"

#: ../../source/advanced/npu_training.rst:85
msgid "拉取昇腾 LLaMA-Factory的 `预构建镜像 <https://hub.docker.com/r/hiyouga/llamafactory/tags>`_ ，请根据实际需要选择合适的版本标签，建议选取最新版本以获得更好的功能支持和性能表现："
msgstr "Pull the Ascend LLaMA-Factory `pre-built image <https://hub.docker.com/r/hiyouga/llamafactory/tags>`_. Please select the appropriate version tag according to your needs. It is recommended to choose the latest version for better feature support and performance:"

#: ../../source/advanced/npu_training.rst:97
msgid "启动容器："
msgstr "Start container:"

#: ../../source/advanced/npu_training.rst:129
msgid "进入容器："
msgstr "Enter container:"

#: ../../source/advanced/npu_training.rst:136
msgid "**NPU 设备挂载说明**："
msgstr "**NPU Device Mount Instructions**:"

#: ../../source/advanced/npu_training.rst:138
msgid "通过 ``--device /dev/davinci<N>`` 参数可挂载指定的 NPU 卡，最多可挂载全部 8 卡"
msgstr "NPU cards can be mounted using the ``--device /dev/davinci<N>`` parameter, up to all 8 cards can be mounted"

#: ../../source/advanced/npu_training.rst:139
msgid "昇腾 NPU 设备从 0 开始编号，容器内的设备编号会自动重新映射"
msgstr "Ascend NPU devices are numbered from 0, and device numbers within the container are automatically remapped"

#: ../../source/advanced/npu_training.rst:140
msgid "例如：将物理机上的 davinci6 和 davinci7 挂载到容器，容器内对应的设备编号将为 0 和 1"
msgstr "For example: When mounting davinci6 and davinci7 from the physical machine to the container, the corresponding device numbers in the container will be 0 and 1"

#: ../../source/advanced/npu_training.rst:142
msgid "进入容器后，LLaMA-Factory 已预装完成，可直接使用 ``llamafactory-cli train`` 命令启动训练。该命令会自动识别容器内所有挂载的 NPU 设备并启用分布式训练："
msgstr "After entering the container, LLaMA-Factory is pre-installed. You can directly use the ``llamafactory-cli train`` command to start training. This command will automatically recognize all mounted NPU devices in the container and enable distributed training:"

#: ../../source/advanced/npu_training.rst:148
msgid "部分微调输出如下所示："
msgstr "Partial fine-tuning output is shown below:"

#: ../../source/advanced/npu_training.rst:171
msgid "若需指定特定的 NPU 设备进行训练（如仅使用卡 0 和卡 1），可通过 ``ASCEND_RT_VISIBLE_DEVICES`` 环境变量进行配置："
msgstr "If you need to specify specific NPU devices for training (such as using only card 0 and card 1), you can configure it through the ``ASCEND_RT_VISIBLE_DEVICES`` environment variable:"

#: ../../source/advanced/npu_training.rst:178
msgid "**模型权重下载配置**："
msgstr "**Model Weight Download Configuration**:"

#: ../../source/advanced/npu_training.rst:180
msgid "默认情况下，系统会从 `Hugging Face <https://huggingface.co/models>`_ 下载模型权重"
msgstr "By default, the system downloads model weights from `Hugging Face <https://huggingface.co/models>`_"

#: ../../source/advanced/npu_training.rst:181
msgid "如遇到网络访问问题，可设置环境变量 ``export USE_MODELSCOPE_HUB=1`` 切换至 `ModelScope <https://modelscope.cn/models>`_ 镜像源"
msgstr "If you encounter network access issues, you can set the environment variable ``export USE_MODELSCOPE_HUB=1`` to switch to the `ModelScope <https://modelscope.cn/models>`_ mirror"

#: ../../source/advanced/npu_training.rst:184
msgid "多机微调"
msgstr "Multi-machine Fine-tuning"

#: ../../source/advanced/npu_training.rst:187
msgid "多机微调场景下，建议直接在物理机上部署 LLaMA-Factory 以获得更好的资源利用率。请参考 :doc:`NPU安装及配置 <./npu_installation>` 完成基础环境配置。"
msgstr "For multi-machine fine-tuning scenarios, it is recommended to deploy LLaMA-Factory directly on physical machines for better resource utilization. Please refer to :doc:`NPU Installation and Configuration <./npu_installation>` to complete the basic environment configuration."

#: ../../source/advanced/npu_training.rst:193
msgid "**环境配置要求**："
msgstr "**Environment Configuration Requirements**:"

#: ../../source/advanced/npu_training.rst:195
msgid "**指定 NPU 设备**：在每个节点上通过环境变量 ``ASCEND_RT_VISIBLE_DEVICES`` 指定参与训练的 NPU 设备（如 ``export ASCEND_RT_VISIBLE_DEVICES=0,1,2,3``）。若不指定，将默认使用节点上的所有 NPU 设备。"
msgstr "**Specify NPU Devices**: On each node, specify the NPU devices participating in training through the environment variable ``ASCEND_RT_VISIBLE_DEVICES`` (e.g., ``export ASCEND_RT_VISIBLE_DEVICES=0,1,2,3``). If not specified, all NPU devices on the node will be used by default."

#: ../../source/advanced/npu_training.rst:197
msgid "**配置通信网卡**：在每个节点上通过环境变量 ``HCCL_SOCKET_IFNAME`` 指定 HCCL 集合通信使用的网卡接口（如 ``export HCCL_SOCKET_IFNAME=eth0``）。可通过 ``ifconfig`` 命令查看可用网卡名称。"
msgstr "**Configure Communication Network Card**: On each node, specify the network card interface used by HCCL collective communication through the environment variable ``HCCL_SOCKET_IFNAME`` (e.g., ``export HCCL_SOCKET_IFNAME=eth0``). You can view available network card names through the ``ifconfig`` command."

#: ../../source/advanced/npu_training.rst:199
msgid "**启动训练**："
msgstr "**Start Training**:"

#: ../../source/advanced/npu_training.rst:201
msgid "以双节点训练为例，分别在主节点和从节点执行以下命令："
msgstr "Taking dual-node training as an example, execute the following commands on the master node and worker node respectively:"

#: ../../source/advanced/npu_training.rst:213
msgid "**环境变量说明**："
msgstr "**Environment Variable Description**:"

#: ../../source/advanced/npu_training.rst:220
msgid "环境变量"
msgstr "Environment Variable"

#: ../../source/advanced/npu_training.rst:221
msgid "说明"
msgstr "Description"

#: ../../source/advanced/npu_training.rst:222
msgid "``FORCE_TORCHRUN``"
msgstr "``FORCE_TORCHRUN``"

#: ../../source/advanced/npu_training.rst:223
msgid "强制使用 torchrun 启动分布式训练"
msgstr "Force using torchrun to start distributed training"

#: ../../source/advanced/npu_training.rst:224
msgid "``NNODES``"
msgstr "``NNODES``"

#: ../../source/advanced/npu_training.rst:225
msgid "参与训练的节点总数"
msgstr "Total number of nodes participating in training"

#: ../../source/advanced/npu_training.rst:226
msgid "``NODE_RANK``"
msgstr "``NODE_RANK``"

#: ../../source/advanced/npu_training.rst:227
msgid "当前节点的全局排名（主节点为 0，从节点依次递增）"
msgstr "Global rank of the current node (master node is 0, worker nodes increment sequentially)"

#: ../../source/advanced/npu_training.rst:228
msgid "``MASTER_ADDR``"
msgstr "``MASTER_ADDR``"

#: ../../source/advanced/npu_training.rst:229
msgid "主节点的 IP 地址"
msgstr "IP address of the master node"

#: ../../source/advanced/npu_training.rst:230
msgid "``MASTER_PORT``"
msgstr "``MASTER_PORT``"

#: ../../source/advanced/npu_training.rst:231
msgid "主节点用于分布式通信的端口号"
msgstr "Port number used for distributed communication on the master node"

#: ../../source/advanced/npu_training.rst:233
msgid "训练范式"
msgstr "Training Paradigms"

#: ../../source/advanced/npu_training.rst:236
msgid "LLaMA-Factory 在 NPU 上支持多种训练范式，使用方式与 GPU 保持一致。以下是各训练范式的启动示例："
msgstr "LLaMA-Factory supports multiple training paradigms on NPU, with usage consistent with GPU. The following are startup examples for each training paradigm:"

#: ../../source/advanced/npu_training.rst:238
msgid "**预训练（Pre-Training, PT）**"
msgstr "**Pre-Training (PT)**"

#: ../../source/advanced/npu_training.rst:244
msgid "**监督微调（Supervised Fine-Tuning, SFT）**"
msgstr "**Supervised Fine-Tuning (SFT)**"

#: ../../source/advanced/npu_training.rst:250
msgid "**奖励模型训练（Reward Modeling, RM）**"
msgstr "**Reward Modeling (RM)**"

#: ../../source/advanced/npu_training.rst:256
msgid "**直接偏好优化（Direct Preference Optimization, DPO）**"
msgstr "**Direct Preference Optimization (DPO)**"

#: ../../source/advanced/npu_training.rst:262
msgid "参数范式"
msgstr "Parameter Paradigms"

#: ../../source/advanced/npu_training.rst:265
msgid "LLaMA-Factory 在 NPU 上支持多种参数微调策略，使用方式与 GPU 保持一致。以下是各参数范式的启动示例："
msgstr "LLaMA-Factory supports multiple parameter fine-tuning strategies on NPU, with usage consistent with GPU. The following are startup examples for each parameter paradigm:"

#: ../../source/advanced/npu_training.rst:267
msgid "**全参数微调（Full Fine-Tuning）**"
msgstr "**Full Fine-Tuning**"

#: ../../source/advanced/npu_training.rst:269
msgid "对模型的所有参数进行更新："
msgstr "Update all parameters of the model:"

#: ../../source/advanced/npu_training.rst:275
msgid "**冻结微调（Freeze Fine-Tuning）**"
msgstr "**Freeze Fine-Tuning**"

#: ../../source/advanced/npu_training.rst:277
msgid "冻结部分层的参数，仅更新指定层："
msgstr "Freeze parameters of some layers and only update specified layers:"

#: ../../source/advanced/npu_training.rst:283
msgid "**LoRA 微调**"
msgstr "**LoRA Fine-Tuning**"

#: ../../source/advanced/npu_training.rst:285
msgid "使用低秩适配器（Low-Rank Adaptation）进行参数高效微调："
msgstr "Use Low-Rank Adaptation for parameter-efficient fine-tuning:"

#: ../../source/advanced/npu_training.rst:291
msgid "模型合并"
msgstr "Model Merging"

#: ../../source/advanced/npu_training.rst:294
msgid "使用 LoRA 方法训练完成后，会生成相应的适配器权重文件。如需将适配器权重合并到基础模型中，请参考 :doc:`LoRA 模型合并 <../getting_started/merge_lora>` 文档。"
msgstr "After training with the LoRA method, corresponding adapter weight files will be generated. To merge the adapter weights into the base model, please refer to the :doc:`LoRA Model Merging <../getting_started/merge_lora>` documentation."

#: ../../source/advanced/npu_training.rst:297
msgid "分布式策略"
msgstr "Distributed Strategies"

#: ../../source/advanced/npu_training.rst:300
msgid "LLaMA-Factory 在 NPU 上支持多种分布式训练策略，包括 DDP、FSDP 和 DeepSpeed，使用方式与 GPU 保持一致。"
msgstr "LLaMA-Factory supports multiple distributed training strategies on NPU, including DDP, FSDP, and DeepSpeed, with usage consistent with GPU."

#: ../../source/advanced/npu_training.rst:305
msgid "LLaMA-Factory 在 ``examples/deepspeed`` 目录下提供了多种 DeepSpeed 配置文件。NPU 当前已支持 ZeRO Stage 1/2/3 及 Offload 功能，可根据训练需求选择相应配置。"
msgstr "LLaMA-Factory provides various DeepSpeed configuration files in the ``examples/deepspeed`` directory. NPU currently supports ZeRO Stage 1/2/3 and Offload features, and you can choose the corresponding configuration according to your training needs."

#: ../../source/advanced/npu_training.rst:307
msgid "在训练配置文件中添加 ``deepspeed`` 参数即可启用 DeepSpeed："
msgstr "Enable DeepSpeed by adding the ``deepspeed`` parameter in the training configuration file:"

#: ../../source/advanced/npu_training.rst:324
msgid "LLaMA-Factory 通过 Accelerate 库支持 FSDP（Fully Sharded Data Parallel）分布式训练。Torch-NPU 已实现与 PyTorch FSDP 接口的兼容，可通过以下命令启动 FSDP 训练："
msgstr "LLaMA-Factory supports FSDP (Fully Sharded Data Parallel) distributed training through the Accelerate library. Torch-NPU has implemented compatibility with the PyTorch FSDP interface. You can start FSDP training with the following command:"

#: ../../source/advanced/npu_training.rst:331
msgid "若需指定特定 NPU 设备，可通过 ``ASCEND_RT_VISIBLE_DEVICES`` 环境变量进行配置："
msgstr "If you need to specify specific NPU devices, you can configure it through the ``ASCEND_RT_VISIBLE_DEVICES`` environment variable:"

#: ../../source/advanced/npu_training.rst:340
msgid "LLaMA-Factory 提供了多种 FSDP 配置文件供选择："
msgstr "LLaMA-Factory provides multiple FSDP configuration files to choose from:"

#: ../../source/advanced/npu_training.rst:342
msgid "``examples/accelerate/fsdp_config.yaml``：标准 FSDP 配置"
msgstr "``examples/accelerate/fsdp_config.yaml``: Standard FSDP configuration"

#: ../../source/advanced/npu_training.rst:343
msgid "``examples/accelerate/fsdp_config_offload.yaml``：支持 CPU Offload 的 FSDP 配置"
msgstr "``examples/accelerate/fsdp_config_offload.yaml``: FSDP configuration with CPU Offload support"

#: ../../source/advanced/npu_training.rst:346
msgid "性能优化"
msgstr "Performance Optimization"

#: ../../source/advanced/npu_training.rst:349
msgid "本节介绍 NPU 训练场景下已验证的性能优化方法。更多优化特性将在验证后持续更新。"
msgstr "This section introduces verified performance optimization methods for NPU training scenarios. More optimization features will be continuously updated after verification."

#: ../../source/advanced/npu_training.rst:354
msgid "LLaMA-Factory 已支持昇腾 NPU 的 FA 融合算子，可显著提升训练性能。在训练配置文件中设置 ``flash_attn: fa2`` 即可启用："
msgstr "LLaMA-Factory supports the FA fused operators of Ascend NPU, which can significantly improve training performance. Enable it by setting ``flash_attn: fa2`` in the training configuration file:"

#: ../../source/advanced/npu_training.rst:370
msgid "融合算子优化仅在训练阶段生效，推理阶段不启用。系统会自动检测并替换相应的算子实现。"
msgstr "Fused operator optimization only takes effect during the training phase and is not enabled during inference. The system will automatically detect and replace the corresponding operator implementation."

#: ../../source/advanced/npu_training.rst:372
msgid "算子下发优化"
msgstr "Operator Dispatch Optimization"

#: ../../source/advanced/npu_training.rst:375
msgid "昇腾提供了 ``TASK_QUEUE_ENABLE`` 环境变量用于优化算子下发性能，可通过流水线并行机制降低算子下发延迟："
msgstr "Ascend provides the ``TASK_QUEUE_ENABLE`` environment variable to optimize operator dispatch performance, which can reduce operator dispatch latency through pipeline parallelism:"

#: ../../source/advanced/npu_training.rst:381
msgid "**配置选项说明**："
msgstr "**Configuration Options Description**:"

#: ../../source/advanced/npu_training.rst:383
msgid "**Level 0（TASK_QUEUE_ENABLE=0）**：关闭 task_queue 算子下发队列优化"
msgstr "**Level 0 (TASK_QUEUE_ENABLE=0)**: Disable task_queue operator dispatch queue optimization"

#: ../../source/advanced/npu_training.rst:385
msgid "**Level 1（TASK_QUEUE_ENABLE=1，默认值）**：启用基础的 task_queue 优化，将算子下发任务分为两级流水线："
msgstr "**Level 1 (TASK_QUEUE_ENABLE=1, default)**: Enable basic task_queue optimization, dividing operator dispatch tasks into two-level pipeline:"

#: ../../source/advanced/npu_training.rst:387
msgid "一级流水：处理常规算子调用"
msgstr "Level 1 pipeline: Handle regular operator calls"

#: ../../source/advanced/npu_training.rst:388
msgid "二级流水：处理 ACLNN 算子调用"
msgstr "Level 2 pipeline: Handle ACLNN operator calls"

#: ../../source/advanced/npu_training.rst:389
msgid "两级流水通过队列并行执行，部分掩盖下发延迟"
msgstr "The two-level pipeline executes in parallel through queues, partially hiding dispatch latency"

#: ../../source/advanced/npu_training.rst:391
msgid "**Level 2（TASK_QUEUE_ENABLE=2，推荐）**：在 Level 1 基础上进一步优化任务负载均衡："
msgstr "**Level 2 (TASK_QUEUE_ENABLE=2, recommended)**: Further optimize task load balancing based on Level 1:"

#: ../../source/advanced/npu_training.rst:393
msgid "将 workspace 相关任务迁移至二级流水"
msgstr "Migrate workspace-related tasks to level 2 pipeline"

#: ../../source/advanced/npu_training.rst:394
msgid "提供更好的延迟掩盖效果和性能收益"
msgstr "Provides better latency hiding effects and performance gains"

#: ../../source/advanced/npu_training.rst:395
msgid "建议在训练场景中使用该配置以获得最佳性能"
msgstr "Recommended to use this configuration in training scenarios for optimal performance"

