# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2025, LlamaFactory team.
# This file is distributed under the same license as the LLaMA Factory
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2025.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: LLaMA Factory \\n"
"Report-Msgid-Bugs-To: \\n"
"POT-Creation-Date: 2025-03-05 01:10+0800\\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\\n"
"Language: en\\n"
"Language-Team: en <LL@li.org>\\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\\n"
"MIME-Version: 1.0\\n"
"Content-Type: text/plain; charset=utf-8\\n"
"Content-Transfer-Encoding: 8bit\\n"
"Generated-By: Babel 2.16.0\\n"

#: ../../source/advanced/npu_training.rst:1
msgid "NPU训练"
msgstr "NPU Training"

#: ../../source/advanced/npu_training.rst:4
msgid "本文档介绍如何在华为昇腾 NPU 上进行 LLaMA-Factory 模型训练。"
msgstr "This document describes how to perform LLaMA-Factory model training on Huawei Ascend NPU."

#: ../../source/advanced/npu_training.rst:6
msgid "支持设备"
msgstr "Supported Devices"

#: ../../source/advanced/npu_training.rst:9
msgid "LLaMA-Factory 当前已适配以下昇腾 NPU 设备："
msgstr "LLaMA-Factory currently supports the following Ascend NPU devices:"

#: ../../source/advanced/npu_training.rst:11
msgid "**Atlas A2 训练系列**"
msgstr "**Atlas A2 Training Series**"

#: ../../source/advanced/npu_training.rst:12
msgid "**Atlas A3 训练系列**"
msgstr "**Atlas A3 Training Series**"

#: ../../source/advanced/npu_training.rst:14
msgid "支持功能"
msgstr "Supported Features"

#: ../../source/advanced/npu_training.rst:23
msgid "功能"
msgstr "Feature"

#: ../../source/advanced/npu_training.rst:24
msgid "支持情况"
msgstr "Support Status"

#: ../../source/advanced/npu_training.rst:26
msgid "**训练范式**"
msgstr "**Training Paradigm**"

#: ../../source/advanced/npu_training.rst:27
msgid "PT"
msgstr "PT"

#: ../../source/advanced/npu_training.rst:28
msgid "已支持"
msgstr "Supported"

#: ../../source/advanced/npu_training.rst:29
msgid "SFT"
msgstr "SFT"

#: ../../source/advanced/npu_training.rst:32
msgid "RM"
msgstr "RM"

#: ../../source/advanced/npu_training.rst:35
msgid "DPO"
msgstr "DPO"

#: ../../source/advanced/npu_training.rst:38
msgid "**参数范式**"
msgstr "**Parameter Paradigm**"

#: ../../source/advanced/npu_training.rst:39
msgid "Full"
msgstr "Full"

#: ../../source/advanced/npu_training.rst:41
msgid "Freeze"
msgstr "Freeze"

#: ../../source/advanced/npu_training.rst:44
msgid "LoRA"
msgstr "LoRA"

#: ../../source/advanced/npu_training.rst:46
msgid "**模型合并**"
msgstr "**Model Merging**"

#: ../../source/advanced/npu_training.rst:47
msgid "LoRA权重合并"
msgstr "LoRA Weight Merging"

#: ../../source/advanced/npu_training.rst:49
msgid "**分布式**"
msgstr "**Distributed**"

#: ../../source/advanced/npu_training.rst:50
msgid "DDP"
msgstr "DDP"

#: ../../source/advanced/npu_training.rst:53
msgid "FSDP"
msgstr "FSDP"

#: ../../source/advanced/npu_training.rst:56
msgid "FSDP2"
msgstr "FSDP2"

#: ../../source/advanced/npu_training.rst:59
msgid "DeepSpeed"
msgstr "DeepSpeed"

#: ../../source/advanced/npu_training.rst:61
msgid "**加速**"
msgstr "**Acceleration**"

#: ../../source/advanced/npu_training.rst:62
msgid "融合算子"
msgstr "Fused Operators"

#: ../../source/advanced/npu_training.rst:63
msgid "当前已支持NpuFusedRMSNorm，NpuFusedSwiGlu，NpuFusedRoPE，NpuFusedMoE"
msgstr "Currently supports NpuFusedRMSNorm, NpuFusedSwiGlu, NpuFusedRoPE, NpuFusedMoE"

#: ../../source/advanced/npu_training.rst:65
msgid "NPU 的大部分使用方式与 GPU 保持一致。关于通用的安装步骤，请参考 :doc:`NPU 安装及配置 <npu_installation>`；关于通用的分布式训练（如 FSDP、FSDP2，DeepSpeed）配置，请参考 :doc:`分布式训练 <distributed>`。"
msgstr "Most NPU usage is consistent with GPU usage. For general installation steps, refer to :doc:`NPU Installation and Configuration <npu_installation>`; for general distributed training configurations (such as FSDP, FSDP2, DeepSpeed), refer to :doc:`Distributed Training <distributed>`."

#: ../../source/advanced/npu_training.rst:68
msgid "快速开始"
msgstr "Quick Start"

#: ../../source/advanced/npu_training.rst:71
msgid "为了快速上手，建议直接使用 LLaMA-Factory 提供的 Docker 镜像。"
msgstr "To get started quickly, it is recommended to use the Docker image provided by LLaMA-Factory."

#: ../../source/advanced/npu_training.rst:73
msgid "**启动容器** (请根据实际情况修改 ``device`` 映射)："
msgstr "**Start the container** (modify the ``device`` mapping as needed):"

#: ../../source/advanced/npu_training.rst:90
msgid "**配置环境变量**："
msgstr "**Configure environment variables**:"

#: ../../source/advanced/npu_training.rst:92
msgid "进入容器后，**务必** 先加载 Ascend 环境配置，否则无法识别 NPU 设备："
msgstr "After entering the container, **be sure** to load the Ascend environment configuration first; otherwise, NPU devices will not be recognized."

#: ../../source/advanced/npu_training.rst:99
msgid "**开始训练**："
msgstr "**Start training**:"

#: ../../source/advanced/npu_training.rst:105
msgid "分布式训练"
msgstr "Distributed Training"

#: ../../source/advanced/npu_training.rst:108
msgid "NPU 的分布式训练配置与 :doc:`分布式训练 <distributed>` 文档描述的基本一致。本节主要介绍 NPU 环境下的特定配置，包括设备指定和多机通信设置。"
msgstr "NPU distributed training configuration is generally consistent with the :doc:`Distributed Training <distributed>` document. This section introduces NPU-specific configurations, including device selection and multi-node communication settings."

#: ../../source/advanced/npu_training.rst:110
msgid "关键环境变量"
msgstr "Key Environment Variables"

#: ../../source/advanced/npu_training.rst:113
msgid "在启动训练前，请注意以下环境变量的设置："
msgstr "Before starting training, pay attention to the following environment variable settings:"

#: ../../source/advanced/npu_training.rst:115
msgid "**ASCEND_RT_VISIBLE_DEVICES** (单机/多机均需关注)"
msgstr "**ASCEND_RT_VISIBLE_DEVICES** (required for single-node and multi-node)"

#: ../../source/advanced/npu_training.rst:117
msgid "用于指定参与训练的 NPU 设备。"
msgstr "Used to specify NPU devices participating in training."

#: ../../source/advanced/npu_training.rst:119
msgid "**默认行为**：如果不设置此变量，程序将尝试使用当前节点上的**所有** NPU 设备。"
msgstr "**Default behavior**: If this variable is not set, the program will attempt to use **all** NPU devices on the current node."

#: ../../source/advanced/npu_training.rst:120
msgid "**指定设备**：如果需要限定特定的 NPU 卡（例如仅使用卡 0 和卡 1），则**必须**显式设置此变量："
msgstr "**Specify devices**: If you need to limit training to specific NPU cards (e.g., only card 0 and card 1), you **must** explicitly set this variable:"

#: ../../source/advanced/npu_training.rst:126
msgid "**HCCL_SOCKET_IFNAME** (仅多机训练必需)"
msgstr "**HCCL_SOCKET_IFNAME** (required for multi-node training only)"

#: ../../source/advanced/npu_training.rst:128
msgid "指定 HCCL 集合通信使用的网卡接口名称。"
msgstr "Specifies the network interface name used for HCCL collective communication."

#: ../../source/advanced/npu_training.rst:130
msgid "**获取方式**：在终端运行 ``ifconfig`` 命令查看网卡列表，选择用于通信的网卡名称（如 ``eth0``, ``enp1s0`` 等）。"
msgstr "**How to obtain**: Run ``ifconfig`` in the terminal to view the network interface list, and choose the interface used for communication (e.g., ``eth0``, ``enp1s0``)."

#: ../../source/advanced/npu_training.rst:131
msgid "**设置示例**："
msgstr "**Example setting**:"

#: ../../source/advanced/npu_training.rst:137
msgid "单机训练"
msgstr "Single-Node Training"

#: ../../source/advanced/npu_training.rst:140
msgid "单机训练（单卡或多卡）的启动方式与标准流程一致。"
msgstr "Single-node training (single card or multiple cards) follows the standard procedure."

#: ../../source/advanced/npu_training.rst:142
msgid "**单机多卡示例**："
msgstr "**Single-node multi-card example**:"

#: ../../source/advanced/npu_training.rst:148
msgid "多机训练"
msgstr "Multi-node Training"

#: ../../source/advanced/npu_training.rst:151
msgid "在 NPU 环境下，推荐使用 ``accelerate launch`` 配合 FSDP 1/2 进行多机训练，这种方式在 NPU 上通信和计算效率更优。"
msgstr "In NPU environments, it is recommended to use ``accelerate launch`` with FSDP 1/2 for multi-node training; this approach provides better communication and computation efficiency on NPU."

#: ../../source/advanced/npu_training.rst:153
msgid "其他启动方式（如 ``torchrun/deepspeed``）及更多详细配置请参考 :doc:`分布式训练 <./distributed>` 文档。"
msgstr "For other launch methods (such as ``torchrun/deepspeed``) and more detailed configurations, refer to the :doc:`Distributed Training <./distributed>` document."

#: ../../source/advanced/npu_training.rst:156
msgid "**1. 准备 Accelerate 配置文件**"
msgstr "**1. Prepare Accelerate configuration file**"

#: ../../source/advanced/npu_training.rst:158
msgid "创建或修改 ``examples/accelerate/fsdp_config.yaml``，关键参数如下（请根据实际节点数和 IP 修改）："
msgstr "Create or modify ``examples/accelerate/fsdp_config.yaml``; key parameters are as follows (please modify according to the actual number of nodes and IP):"

#: ../../source/advanced/npu_training.rst:187
msgid "关键多机参数说明："
msgstr "Explanation of key multi-node parameters:"

#: ../../source/advanced/npu_training.rst:191
msgid "num_machines: 节点总数"
msgstr "num_machines: Total number of nodes"

#: ../../source/advanced/npu_training.rst:192
msgid "num_processes: 总进程数（总卡数） = num_machines * 每台机器卡数"
msgstr "num_processes: Total number of processes (total number of cards) = num_machines * number of cards per machine"

#: ../../source/advanced/npu_training.rst:193
msgid "main_process_ip: 主节点 IP 地址（所有节点需保持一致）"
msgstr "main_process_ip: IP address of the main node (must be consistent across all nodes)"

#: ../../source/advanced/npu_training.rst:194
msgid "main_process_port: 主节点端口（所有节点需保持一致）"
msgstr "main_process_port: Port of the main node (must be consistent across all nodes)"

#: ../../source/advanced/npu_training.rst:195
msgid "machine_rank: 当前节点编号（主节点为0，从节点依次递增）"
msgstr "machine_rank: Current node index (main node is 0, and the subsequent nodes increase in order)"

#: ../../source/advanced/npu_training.rst:197
msgid "**2. 启动训练**"
msgstr "**2. Launch training**"

#: ../../source/advanced/npu_training.rst:199
msgid "在所有节点上执行相同的启动命令（确保 ``machine_rank`` 在 yaml 中已正确配置）："
msgstr "Execute the same launch command on all nodes (ensure ``machine_rank`` is correctly configured in the YAML):"

#: ../../source/advanced/npu_training.rst:208
msgid "训练方式"
msgstr "Training Modes"

#: ../../source/advanced/npu_training.rst:211
msgid "以下是常见训练场景的启动命令参考，具体参数配置文件请根据实际需求调整。"
msgstr "The following are reference launch commands for common training scenarios; adjust the configuration files according to your needs."

#: ../../source/advanced/npu_training.rst:213
msgid "预训练 (PT)"
msgstr "Pretraining (PT)"

#: ../../source/advanced/npu_training.rst:220
msgid "监督微调 (SFT)"
msgstr "Supervised Fine-tuning (SFT)"

#: ../../source/advanced/npu_training.rst:227
msgid "奖励模型 (RM)"
msgstr "Reward Model (RM)"

#: ../../source/advanced/npu_training.rst:234
msgid "DPO 训练"
msgstr "DPO Training"

#: ../../source/advanced/npu_training.rst:241
msgid "全参数微调 (Full)"
msgstr "Full-parameter Fine-tuning (Full)"

#: ../../source/advanced/npu_training.rst:248
msgid "性能优化"
msgstr "Performance Optimization"

#: ../../source/advanced/npu_training.rst:251
msgid "融合算子"
msgstr "Fused Operators"

#: ../../source/advanced/npu_training.rst:254
msgid "LLaMA-Factory 支持FA，NpuFusedRMSNorm，NpuFusedSwiGlu，NpuFusedRoPE和NpuFusedMoE融合算子。"
msgstr "LLaMA-Factory supports FA, NpuFusedRMSNorm, NpuFusedSwiGlu, NpuFusedRoPE, and NpuFusedMoE fused operators."

#: ../../source/advanced/npu_training.rst:256
msgid "可在训练脚本中配置如下参数，模型加载后替换对应模型结构，使能NpuFusedRMSNorm，NpuFusedSwiGlu，NpuFusedRoPE和NpuFusedMoE融合算子，提升训练效率。该接口使能后，代码内部自动识别是否满足模型结构替换的要求，满足的情况对应模型结构会被替换为融合算子形式。"
msgstr "Configure the following parameter in the training script to enable NpuFusedRMSNorm, NpuFusedSwiGlu, NpuFusedRoPE, and NpuFusedMoE fused operators, replacing the corresponding model structures after loading to improve training efficiency. After enabling this interface, the code automatically determines whether the model structure meets the replacement requirements. If satisfied, the corresponding model structure will be replaced with the fused operator form."

#: ../../source/advanced/npu_training.rst:263
msgid "同时LLaMA-Factory 支持昇腾 NPU 的 FA 融合算子，代码内部自动识别是否满足模型结构替换的要求，满足的情况对应模型结构会被替换为融合算子形式。在训练配置文件中设置如下参数即可使能："
msgstr "LLaMA-Factory also supports FA fused operators on Ascend NPU. The code automatically determines whether the model structure meets the replacement requirements, and will replace the structure with the fused operator form if satisfied. Enable it by setting the following parameter in the training configuration file:"

#: ../../source/advanced/npu_training.rst:269
msgid "当前融合算子对模型的支持程度受限，该功能正在持续迭代开发中，以提升泛化性和适用性。"
msgstr "Currently, the range of model support for fused operators is limited. This feature is under continuous development to improve generality and applicability."

#: ../../source/advanced/npu_training.rst:276
msgid "支持模型系列"
msgstr "Supported Model Series"

#: ../../source/advanced/npu_training.rst:278
msgid "FA"
msgstr "FA"

#: ../../source/advanced/npu_training.rst:280
msgid "NpuFusedRMSNorm"
msgstr "NpuFusedRMSNorm"

#: ../../source/advanced/npu_training.rst:283
msgid "NpuFusedSwiGlu"
msgstr "NpuFusedSwiGlu"

#: ../../source/advanced/npu_training.rst:285
msgid "NpuFusedRoPE"
msgstr "NpuFusedRoPE"

#: ../../source/advanced/npu_training.rst:287
msgid "NpuFusedMoE"
msgstr "NpuFusedMoE"

#: ../../source/advanced/npu_training.rst:290
msgid "算子下发优化"
msgstr "Operator Dispatch Optimization"

#: ../../source/advanced/npu_training.rst:293
msgid "通过设置 ``TASK_QUEUE_ENABLE`` 环境变量优化算子下发性能（推荐 Level 2）："
msgstr "Optimize operator dispatch performance by setting the ``TASK_QUEUE_ENABLE`` environment variable (Level 2 recommended):"
