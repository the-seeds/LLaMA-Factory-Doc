# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2025, LlamaFactory team.
# This file is distributed under the same license as the LLaMA Factory
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2025.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PROJECT VERSION\n"
"Report-Msgid-Bugs-To: EMAIL@ADDRESS\n"
"POT-Creation-Date: 2025-03-05 01:10+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.16.0\n"

#: ../../source/finetune_best_practices.rst:5
msgid "3步实现 GPT-OSS 的 LoRA 微调"
msgstr "3 Steps to LoRA Fine-tuning for GPT-OSS"

#: ../../source/finetune_best_practices.rst:8
msgid "1. 安装 LLaMA-Factory 和 transformers"
msgstr "1. Install LLaMA-Factory and transformers"

#: ../../source/finetune_best_practices.rst:15
msgid "2. 在单张 GPU 上训练 GPT-OSS（要求显存 > 44 GB, 支持多 GPU）"
msgstr "2. Train GPT-OSS on a single GPU (requires VRAM > 44 GB, multi-GPU supported)"

#: ../../source/finetune_best_practices.rst:22
msgid "3. 合并 LoRA 权重"
msgstr "3. Merge LoRA Weights"

#: ../../source/finetune_best_practices.rst:29
msgid "与微调后的模型进行对话"
msgstr "Chat with the Fine-tuned Model"

#: ../../source/finetune_best_practices.rst:36
msgid "全量微调脚本"
msgstr "Full Fine-tuning Script"

#: ../../source/finetune_best_practices.rst:85
msgid "训练损失曲线"
msgstr "Training Loss Curve"

#: ../../source/finetune_best_practices.rst:89
msgid "使用 Web UI 微调模型："
msgstr "Fine-tune the Model via Web UI:"

#: ../../source/finetune_best_practices.rst:93
msgid "使用 Web UI 微调 gpt-oss"
msgstr "Fine-tune gpt-oss via Web UI"
