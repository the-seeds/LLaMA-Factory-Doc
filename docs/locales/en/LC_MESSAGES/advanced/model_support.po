# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2025, LlamaFactory team.
# This file is distributed under the same license as the LLaMA Factory
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2025.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PROJECT VERSION\n"
"Report-Msgid-Bugs-To: EMAIL@ADDRESS\n"
"POT-Creation-Date: 2025-03-05 01:10+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.16.0\n"

#: model_support.rst
msgid "模型支持"
msgstr "Model Support"

#: model_support.rst
msgid "LLaMA-Factory 允许用户添加自定义模型支持。我们将以 LLaMA-4 多模态模型为例，详细介绍如何为新模型添加支持。对于多模态模型，我们需要完成两个主要任务："
msgstr "LLaMA-Factory allows users to add support for custom models. We will take the LLaMA-4 multimodal model as an example to introduce how to add support for a new model. For multimodal models, we need to complete two main tasks:"

#: model_support.rst
msgid "注册模型的template"
msgstr "Register the model's template"

#: model_support.rst
msgid "解析多模态数据并构建 messages"
msgstr "Parse multimodal data and construct messages"

#: model_support.rst
msgid "注册 template"
msgstr "Register Template"

#: model_support.rst
msgid "首先，我们可以通过以下方法获取 LLaMA-4 模型的 template："
msgstr "First, we can obtain the template of the LLaMA-4 model using the following method:"

#: model_support.rst
msgid "输出如下。通过观察输出我们可以得到模型的 chat_template。除此以外也可以通过 `huggingface repo <https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct/blob/main/tokenizer_config.json#L9077>`_  来获取模型的 template."
msgstr "The output is as follows. By observing the output, we can obtain the model's chat_template. Additionally, you can also obtain the model's template via the `huggingface repo <https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct/blob/main/tokenizer_config.json#L9077>`_."

#: model_support.rst
msgid "通过观察输出，我们可以得知 LLaMA-4 的 chat_template 主要由以下几部分组成："
msgstr "By observing the output, we can see that the chat_template of LLaMA-4 mainly consists of the following parts:"

#: model_support.rst
msgid "消息类型"
msgstr "Message Type"

#: model_support.rst
msgid "模板格式"
msgstr "Template Format"

#: model_support.rst
msgid "用户消息"
msgstr "User message"

#: model_support.rst
msgid "``<|header_start|>user<|header_end|>\\n\\n{{content}}<|eot|>``"
msgstr "``<|header_start|>user<|header_end|>\\n\\n{{content}}<|eot|>``"

#: model_support.rst
msgid "助手消息"
msgstr "Assistant message"


#: model_support.rst
msgid "系统消息"
msgstr "System message"

#: model_support.rst
msgid "``<|header_start|>system<|header_end|>\\n\\n{{content}}<|eot|>``"
msgstr "``<|header_start|>system<|header_end|>\\n\\n{{content}}<|eot|>``"

#: model_support.rst
msgid "工具消息"
msgstr "Tool message"

#: model_support.rst
msgid "``<|header_start|>ipython<|header_end|>\\n\\n\"{{content}}\"<|eot|>``"
msgstr "``<|header_start|>ipython<|header_end|>\\n\\n\"{{content}}\"<|eot|>``"


#: model_support.rst
msgid "我们可以在 ``src/llamafactory/data/template.py`` 中使用 ``register_template`` 方法为自定义模型注册 chat_template。"
msgstr "We can use the ``register_template`` method in ``src/llamafactory/data/template.py`` to register the chat_template for the custom model."

#: model_support.rst
msgid "在实际应用中，我们往往会在用户输入的信息后添加助手回复模板的头部 ``<|header_start|>assistant<|header_end|>`` 来引导模型进行回复。"
msgstr "In practice, we often append the assistant response header ``<|header_start|>assistant<|header_end|>`` after the user's input to guide the model's response."

#: model_support.rst
msgid "因此我们可以看到，用户消息和工具输出的模板中都附有了助手回复的头部，而助手消息格式 ``format_assitant`` 也因此省略了助手回复的头部，只保留其内容部分 ``{{content}}<|eot|。>``。"
msgstr "Therefore, we can see that the user message and tool output templates both include the assistant response header, and thus the assistant message format ``format_assistant`` omits the assistant response header, keeping only its content part ``{{content}}<|eot|>``."

#: model_support.rst
msgid "我们可以根据上面的输出完成 ``name``, ``format_user``, ``format_assistant``, ``format_system`` 与 ``format_observation`` 字段的填写。"
msgstr "We can complete the ``name``, ``format_user``, ``format_assistant``, ``format_system``, and ``format_observation`` fields based on the output above."

#: model_support.rst
msgid "``format_prefix`` 字段用于指定模型的开头部分，通常可以在 `tokenizer_config.json <https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct/blob/main/tokenizer_config.json#L9076>`_ 中找到。"
msgstr "The ``format_prefix`` field specifies the beginning of the model sequence and can usually be found in `tokenizer_config.json <https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct/blob/main/tokenizer_config.json#L9076>`_."

#: model_support.rst
msgid "``stop_words`` 字段用于指定模型的停止词，可以在 `generation_config.json <https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct/blob/main/generation_config.json>`_ 中找到 eos_token_id，再把 eos_token_id 对应的 token 填入。"
msgstr "The ``stop_words`` field specifies the model's stop words. You can find the eos_token_id in `generation_config.json <https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct/blob/main/generation_config.json>`_ and fill in the corresponding token."

#: model_support.rst
msgid "对于多模态模型，我们还需要在 ``mm_plugin`` 字段中指定多模态插件。"
msgstr "For multimodal models, we also need to specify the multimodal plugin in the ``mm_plugin`` field."

#: model_support.rst
msgid "多模态数据构建"
msgstr "Multimodal Data Construction"

#: model_support.rst
msgid "对于多模态模型，我们参照原始模型在 LLaMA-Factory 中实现多模态数据的解析。"
msgstr "For multimodal models, we refer to the original model to implement multimodal data parsing in LLaMA-Factory."

#: model_support.rst
msgid "我们可以在 ``src/llamafactory/data/mm_plugin.py`` 中实现 ``Llama4Plugin`` 类来解析多模态数据。"
msgstr "We can implement the ``Llama4Plugin`` class in ``src/llamafactory/data/mm_plugin.py`` to parse multimodal data."

#: model_support.rst
msgid "``Llama4Plugin`` 类继承自 ``BasePlugin`` 类，并实现了 ``get_mm_inputs`` 和 ``process_messages`` 方法来解析多模态数据。"
msgstr "The ``Llama4Plugin`` class inherits from the ``BasePlugin`` class and implements the ``get_mm_inputs`` and ``process_messages`` methods to parse multimodal data."

#: model_support.rst
msgid "``get_mm_inputs`` 的作用是将图像、视频等多模态数据转化为模型可以接收的输入，如 ``pixel_values``。为实现 ``get_mm_inputs``，首先我们需要检查 llama4 的 processor 是否可以与 `已有实现 <https://github.com/hiyouga/LLaMA-Factory/blob/da971c37640de20f97b4d774e77e6f8d5c00b40a/src/llamafactory/data/mm_plugin.py#L264>`_ 兼容。模型官方仓库中的 `processing_llama4.py <https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama4/processing_llama4.py#L157>`_ 表明 llama4 的 processor 返回数据包含字段 ``pixel_values``，这与 LLaMA-Factory 中的已有实现兼容。因此，我们只需要参照已有的 ``get_mm_inputs`` 方法实现即可。"
msgstr "The function of ``get_mm_inputs`` is to convert multimodal data such as images and videos into inputs that the model can accept, such as ``pixel_values``. To implement ``get_mm_inputs``, we first need to check if the llama4 processor is compatible with the `existing implementation <https://github.com/hiyouga/LLaMA-Factory/blob/da971c37640de20f97b4d774e77e6f8d5c00b40a/src/llamafactory/data/mm_plugin.py#L264>`_. The `processing_llama4.py <https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama4/processing_llama4.py#L157>`_ in the official model repository indicates that the data returned by the llama4 processor contains the field ``pixel_values``, which is compatible with the existing implementation in LLaMA-Factory. Therefore, we only need to implement it by referring to the existing ``get_mm_inputs`` method."

#: model_support.rst
msgid "``process_messages`` 的作用是根据输入图片/视频的大小，数量等信息在 messages 中插入相应数量的占位符，以便模型可以正确解析多模态数据。 我们需要参考 `原仓库实现 <https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama4/processing_llama4.py#L157>`_ 以及 LLaMA-Factory 中的规范返回 ``list[dict[str, str]]`` 类型的 messages。"
msgstr "The function of ``process_messages`` is to insert a corresponding number of placeholders in the messages based on the size, quantity, etc., of the input images/videos so that the model can correctly parse the multimodal data. We need to refer to the `original repository implementation <https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama4/processing_llama4.py#L157>`_ and the specifications in LLaMA-Factory to return messages of type ``list[dict[str, str]]``."

#: model_support.rst
msgid "提供模型路径"
msgstr "Provide Model Path"

#: model_support.rst
msgid "最后, 在 `src/llamafactory/extras/constants.py <https://github.com/hiyouga/LLaMA-Factory/blob/main/src/llamafactory/extras/constants.py>`_ 中提供模型的下载路径。例如："
msgstr "Finally, provide the model download path in `src/llamafactory/extras/constants.py <https://github.com/hiyouga/LLaMA-Factory/blob/main/src/llamafactory/extras/constants.py>`_. For example:"